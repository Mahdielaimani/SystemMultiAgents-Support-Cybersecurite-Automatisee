# Int√©gration de vos mod√®les Hugging Face pour la cybers√©curit√©
# Mod√®les: elmahdielmani/vulnerability-classifier, elmahdielmani/network-analyzer-cicids, elmahdielmani/intent-classifier-security

import logging
import os
from typing import Dict, List, Any, Optional
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import numpy as np
from datetime import datetime

logger = logging.getLogger(__name__)

class HuggingFaceSecurityModels:
    """Gestionnaire de vos mod√®les Hugging Face de cybers√©curit√©"""

    def __init__(self, your_token_here: str = None):
        self.your_token_here = your_token_here or "your_token_here"
        self.models = {}
        self.tokenizers = {}
        self.pipelines = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # Configuration de vos mod√®les r√©els
        self.model_configs = {
            "vulnerability_classifier": {
                "model_name": "elmahdielmani/vulnerability-classifier",
                "local_path": "./models/vulnerability_classifier/",
                "task": "text-classification",
                "description": "Classifie les types de vuln√©rabilit√©s dans le code/texte"
            },
            "network_analyzer": {
                "model_name": "elmahdielmani/network-analyzer-cicids",
                "local_path": "./models/network_analyzer/",
                "task": "text-classification",
                "description": "Analyse le trafic r√©seau pour d√©tecter les attaques (dataset CIC-IDS)"
            },
            "intent_classifier": {
                "model_name": "elmahdielmani/intent-classifier-security",
                "local_path": "./models/intent_classifier/",
                "task": "text-classification",
                "description": "Classifie l'intention de s√©curit√© (malveillante ou l√©gitime)"
            }
        }

        logger.info(f"ü§ñ Initialisation avec vos mod√®les HF - Device: {self.device}")
        self._load_models()

    def _load_models(self):
        """Charge tous vos mod√®les de s√©curit√© depuis Hugging Face"""
        for model_key, config in self.model_configs.items():
            try:
                logger.info(f"üîÑ Chargement de votre mod√®le {model_key} depuis HF...")

                model_name = config["model_name"]

                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    token=self.your_token_here,
                    trust_remote_code=True
                )
                self.tokenizers[model_key] = tokenizer

                model = AutoModelForSequenceClassification.from_pretrained(
                    model_name,
                    token=self.your_token_here,
                    trust_remote_code=True
                )
                model.to(self.device)
                self.models[model_key] = model

                self.pipelines[model_key] = pipeline(
                    config["task"],
                    model=model,
                    tokenizer=tokenizer,
                    device=0 if self.device == "cuda" else -1,
                    token=self.your_token_here
                )

                logger.info(f"‚úÖ Mod√®le {model_key} charg√© avec succ√®s depuis HF Hub")

            except Exception as e:
                logger.error(f"‚ùå Erreur chargement {model_key}: {e}")
                logger.info(f"üîÑ Tentative de fallback pour {model_key}...")
                self._create_intelligent_fallback(model_key, config)

    # ... le reste du code est correct syntaxiquement et peut √™tre ajout√© √† ce fichier ...
    def _create_intelligent_fallback(self, model_key: str, config: Dict[str, Any]):
        """Fallback : t√©l√©charge le mod√®le localement en cas d'√©chec HF direct"""
        try:
            logger.info(f"üì• T√©l√©chargement local de {model_key} depuis {config['model_name']}...")

            tokenizer = AutoTokenizer.from_pretrained(
                config["model_name"],
                cache_dir=config["local_path"],
                token=self.your_token_here,
                trust_remote_code=True
            )
            self.tokenizers[model_key] = tokenizer

            model = AutoModelForSequenceClassification.from_pretrained(
                config["model_name"],
                cache_dir=config["local_path"],
                token=self.your_token_here,
                trust_remote_code=True
            )
            model.to(self.device)
            self.models[model_key] = model

            self.pipelines[model_key] = pipeline(
                config["task"],
                model=model,
                tokenizer=tokenizer,
                device=0 if self.device == "cuda" else -1,
                token=self.your_token_here
            )

            logger.info(f"‚úÖ Fallback r√©ussi pour le mod√®le {model_key}")

        except Exception as fallback_error:
            logger.error(f"‚ùå Fallback √©chou√© pour {model_key}: {fallback_error}")
            raise RuntimeError(f"Impossible de charger ou fallback le mod√®le : {model_key}")

    def predict(self, model_key: str, texts: List[str], top_k: Optional[int] = 1) -> List[Dict[str, Any]]:
        """Effectue une pr√©diction sur une liste de textes avec le mod√®le sp√©cifi√©"""
        if model_key not in self.pipelines:
            raise ValueError(f"Mod√®le non trouv√© ou non charg√© : {model_key}")
        
        logger.info(f"üìä Pr√©diction avec le mod√®le {model_key} sur {len(texts)} textes")
        try:
            results = self.pipelines[model_key](texts, top_k=top_k, truncation=True)
            if isinstance(results[0], dict):
                return [results]  # single text
            return results  # list of results
        except Exception as e:
            logger.error(f"‚ùå Erreur pendant la pr√©diction : {e}")
            raise RuntimeError(f"Erreur de pr√©diction avec le mod√®le {model_key}")
