"""
Agent Support Agentic avec RAG hybride : Vector (ChromaDB) + Graph (Neo4j) + Gemini/Mistral
"""

import logging
import os
import sys
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import json
import re

# Ajouter le r√©pertoire racine au path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# Imports pour RAG Vector
try:
    from sentence_transformers import SentenceTransformer
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  ChromaDB non disponible")
    CHROMADB_AVAILABLE = False

# Imports pour RAG Graph
try:
    from core.graph_manager import KnowledgeGraphManager
    GRAPH_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  KnowledgeGraphManager non disponible")
    GRAPH_AVAILABLE = False

# Imports pour LLM hybride Gemini
try:
    from utils.hybrid_llm_manager_gemini import HybridLLMManagerGemini
    HYBRID_LLM_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  Gestionnaire LLM hybride Gemini non disponible")
    HYBRID_LLM_AVAILABLE = False

@dataclass
class RAGResult:
    """R√©sultat combin√© Vector + Graph RAG"""
    vector_results: List[Dict[str, Any]]
    graph_results: List[Dict[str, Any]]
    combined_context: str
    confidence_score: float

class AgenticSupportAgentRAGGraph:
    """
    Agent Support avec RAG hybride (Vector + Graph) + Gemini/Mistral
    """
    
    def __init__(self):
        self.logger = self._setup_logger()
        
        # Initialisation des composants
        self.embedding_model = None
        self.chroma_client = None
        self.collection = None
        self.graph_manager = None
        self.llm_manager = None
        
        # Patterns de d√©tection
        self.greeting_patterns = [
            r'\b(bonjour|salut|hello|hi|bonsoir|hey|coucou)\b',
            r'\b(comment √ßa va|√ßa va)\b'
        ]
        
        self.pricing_patterns = [
            r'\b(prix|tarif|co√ªt|combien|pricing)\b',
            r'\b(donner.*prix|montrer.*prix|voir.*prix)\b',
            r'\b(plan|abonnement|forfait)\b'
        ]
        
        self.teamsquare_patterns = [
            r'\b(teamsquare|team square)\b',
            r'\b(qu.*est.*teamsquare|c.*est.*quoi.*teamsquare)\b',
            r'\b(fonctionnalit√©|feature|service|api|int√©gration)\b'
        ]
        
        self._initialize_components()
    
    def _setup_logger(self):
        """Configure le logger"""
        logger = logging.getLogger("AgenticSupportAgentRAGGraph")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _initialize_components(self):
        """Initialise tous les composants RAG hybride"""
        try:
            # 1. Gestionnaire LLM hybride Gemini
            if HYBRID_LLM_AVAILABLE:
                self.logger.info("üîÑ Initialisation du gestionnaire LLM hybride Gemini...")
                self.llm_manager = HybridLLMManagerGemini()
                self.logger.info(f"‚úÖ LLM hybride Gemini initialis√© - Provider: {self.llm_manager.current_provider}")
            
            # 2. Mod√®le d'embedding pour Vector RAG
            if CHROMADB_AVAILABLE:
                self.logger.info("üîÑ Chargement du mod√®le BGE-Large-EN...")
                try:
                    self.embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')
                    self.logger.info("‚úÖ Mod√®le BGE-Large-EN charg√©")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è  Erreur chargement BGE: {e}")
                
                # 3. ChromaDB pour Vector RAG
                self.logger.info("üîÑ Connexion √† ChromaDB...")
                try:
                    chroma_path = "./data/vector_db/chroma_db"
                    os.makedirs(chroma_path, exist_ok=True)
                    
                    self.chroma_client = chromadb.PersistentClient(
                        path=chroma_path,
                        settings=Settings(anonymized_telemetry=False)
                    )
                    
                    try:
                        self.collection = self.chroma_client.get_collection("knowledge_base")
                        self.logger.info("‚úÖ Collection ChromaDB trouv√©e")
                    except:
                        self.logger.warning("‚ö†Ô∏è  Collection ChromaDB non trouv√©e")
                        
                except Exception as e:
                    self.logger.error(f"‚ùå Erreur ChromaDB: {e}")
            
            # 4. Neo4j pour Graph RAG
            if GRAPH_AVAILABLE:
                self.logger.info("üîÑ Initialisation du gestionnaire de graphe Neo4j...")
                try:
                    self.graph_manager = KnowledgeGraphManager()
                    self.logger.info("‚úÖ Gestionnaire de graphe Neo4j initialis√©")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è  Erreur Neo4j: {e}")
                    self.graph_manager = None
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation: {e}")
    
    def process_query(self, query: str) -> str:
        """
        Traite une requ√™te avec RAG hybride (Vector + Graph) + LLM
        """
        self.logger.info(f"üîç Traitement requ√™te RAG hybride: {query[:50]}...")
        
        try:
            # 1. D√©tection du type de requ√™te
            query_type = self._detect_query_type(query)
            
            # 2. RAG hybride : Vector + Graph
            rag_result = self._hybrid_rag_search(query, query_type)
            
            # 3. G√©n√©ration avec LLM hybride
            if self.llm_manager and query_type != "greeting":
                response = self._generate_with_llm(query, rag_result, query_type)
            else:
                response = self._generate_fallback(query, rag_result.combined_context, query_type)
            
            # 4. Mise √† jour du graphe de connaissances
            self._update_knowledge_graph(query, response, query_type)
            
            self.logger.info(f"‚úÖ R√©ponse g√©n√©r√©e ({len(response)} chars) - Confiance: {rag_result.confidence_score:.2f}")
            return response
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur traitement requ√™te: {e}")
            return "D√©sol√©, j'ai rencontr√© un probl√®me technique. Pouvez-vous reformuler votre question ?"
    
    def _hybrid_rag_search(self, query: str, query_type: str) -> RAGResult:
        """
        Recherche hybride : Vector RAG (ChromaDB) + Graph RAG (Neo4j)
        """
        self.logger.info("üîç Recherche RAG hybride (Vector + Graph)...")
        
        # R√©sultats par d√©faut
        vector_results = []
        graph_results = []
        
        # 1. Vector RAG avec ChromaDB
        if self.embedding_model and self.collection:
            try:
                self.logger.info("üìä Recherche vectorielle...")
                query_embedding = self.embedding_model.encode([query]).tolist()[0]
                
                chroma_results = self.collection.query(
                    query_embeddings=query_embedding,
                    n_results=5,
                    include=['documents', 'metadatas', 'distances']
                )
                
                if chroma_results['documents'] and chroma_results['documents'][0]:
                    for doc, metadata, distance in zip(
                        chroma_results['documents'][0],
                        chroma_results['metadatas'][0] or [{}] * len(chroma_results['documents'][0]),
                        chroma_results['distances'][0]
                    ):
                        if distance < 0.7:  # Seuil de pertinence
                            vector_results.append({
                                'content': doc,
                                'metadata': metadata,
                                'score': 1 - distance,
                                'source': 'vector_db'
                            })
                
                self.logger.info(f"üìä Trouv√© {len(vector_results)} r√©sultats vectoriels")
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur recherche vectorielle: {e}")
        
        # 2. Graph RAG avec Neo4j
        if self.graph_manager:
            try:
                self.logger.info("üï∏Ô∏è Recherche dans le graphe...")
                
                # Recherche d'entit√©s similaires
                graph_entities = self.graph_manager.search_entities(query, limit=3)
                
                for entity in graph_entities:
                    # R√©cup√©rer le voisinage de chaque entit√©
                    neighborhood = self.graph_manager.get_entity_neighborhood(
                        entity['name'], depth=2
                    )
                    
                    if neighborhood['nodes']:
                        graph_results.append({
                            'entity': entity['name'],
                            'type': entity['type'],
                            'score': entity.get('score', 0.5),
                            'neighborhood': neighborhood,
                            'source': 'knowledge_graph'
                        })
                
                self.logger.info(f"üï∏Ô∏è Trouv√© {len(graph_results)} r√©sultats graphiques")
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur recherche graphique: {e}")
        
        # 3. Combinaison des contextes
        combined_context = self._combine_contexts(vector_results, graph_results, query_type)
        
        # 4. Calcul du score de confiance
        confidence_score = self._calculate_confidence(vector_results, graph_results)
        
        return RAGResult(
            vector_results=vector_results,
            graph_results=graph_results,
            combined_context=combined_context,
            confidence_score=confidence_score
        )
    
    def _combine_contexts(self, vector_results: List[Dict], graph_results: List[Dict], query_type: str) -> str:
        """Combine les contextes vectoriel et graphique"""
        
        context_parts = []
        
        # Contexte par d√©faut
        default_context = self._get_default_context(query_type)
        context_parts.append(f"INFORMATIONS DE BASE:\n{default_context}")
        
        # Contexte vectoriel
        if vector_results:
            vector_context = "\n".join([result['content'] for result in vector_results[:3]])
            context_parts.append(f"DOCUMENTS PERTINENTS:\n{vector_context}")
        
        # Contexte graphique
        if graph_results:
            graph_context = []
            for result in graph_results[:2]:
                entity_info = f"Entit√©: {result['entity']} (Type: {result['type']})"
                
                # Ajouter les relations
                neighborhood = result.get('neighborhood', {})
                if neighborhood.get('relationships'):
                    relations = []
                    for rel in neighborhood['relationships'][:3]:
                        relations.append(f"- {rel['type']}")
                    entity_info += f"\nRelations: {', '.join(relations)}"
                
                graph_context.append(entity_info)
            
            if graph_context:
                context_parts.append(f"RELATIONS DANS LE GRAPHE:\n" + "\n\n".join(graph_context))
        
        return "\n\n".join(context_parts)
    
    def _calculate_confidence(self, vector_results: List[Dict], graph_results: List[Dict]) -> float:
        """Calcule un score de confiance bas√© sur les r√©sultats RAG"""
        
        confidence = 0.0
        
        # Score vectoriel
        if vector_results:
            avg_vector_score = sum(r['score'] for r in vector_results) / len(vector_results)
            confidence += avg_vector_score * 0.6
        
        # Score graphique
        if graph_results:
            avg_graph_score = sum(r['score'] for r in graph_results) / len(graph_results)
            confidence += avg_graph_score * 0.4
        
        # Bonus si les deux sources sont disponibles
        if vector_results and graph_results:
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _update_knowledge_graph(self, query: str, response: str, query_type: str):
        """Met √† jour le graphe de connaissances avec l'interaction"""
        
        if not self.graph_manager:
            return
        
        try:
            self.logger.info("üï∏Ô∏è Mise √† jour du graphe de connaissances...")
            
            # Utiliser la m√©thode existante du graph_manager
            self.graph_manager.update_with_interaction(
                query=query,
                response=response,
                agent_type="agentic_rag_graph"
            )
            
            self.logger.info("‚úÖ Graphe de connaissances mis √† jour")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur mise √† jour graphe: {e}")
    
    def _detect_query_type(self, query: str) -> str:
        """D√©tecte le type de requ√™te"""
        query_lower = query.lower()
        
        if any(re.search(pattern, query_lower, re.IGNORECASE) for pattern in self.greeting_patterns):
            return "greeting"
        elif any(re.search(pattern, query_lower, re.IGNORECASE) for pattern in self.pricing_patterns):
            return "pricing"
        elif any(re.search(pattern, query_lower, re.IGNORECASE) for pattern in self.teamsquare_patterns):
            return "teamsquare"
        else:
            return "general"
    
    def _get_default_context(self, query_type: str) -> str:
        """Contexte par d√©faut bas√© sur le type de requ√™te"""
        
        if query_type == "pricing":
            return """TeamSquare propose trois plans tarifaires :
- Plan Starter : 29‚Ç¨/mois (jusqu'√† 10 utilisateurs, fonctionnalit√©s de base)
- Plan Professional : 79‚Ç¨/mois (jusqu'√† 50 utilisateurs, fonctionnalit√©s avanc√©es, API)
- Plan Enterprise : 199‚Ç¨/mois (utilisateurs illimit√©s, toutes fonctionnalit√©s, support d√©di√©)"""
        
        elif query_type == "teamsquare":
            return """TeamSquare est une plateforme de collaboration moderne qui offre :
- Collaboration en temps r√©el avec chat int√©gr√©
- Gestion de projets intuitive et visuelle
- Partage de fichiers s√©curis√©
- Tableaux de bord et analytics avanc√©s
- API compl√®te pour int√©grations
- S√©curit√© de niveau entreprise"""
        
        else:
            return "Je suis un assistant sp√©cialis√© dans TeamSquare, une plateforme de collaboration pour √©quipes."
    
    def _generate_with_llm(self, query: str, rag_result: RAGResult, query_type: str) -> str:
        """G√©n√®re une r√©ponse avec le gestionnaire LLM hybride"""
        
        system_prompt = f"""Tu es un assistant support intelligent pour TeamSquare, une plateforme de collaboration.

CONTEXTE RAG HYBRIDE (Vector + Graph) :
{rag_result.combined_context}

SCORE DE CONFIANCE : {rag_result.confidence_score:.2f}
SOURCES DISPONIBLES : {len(rag_result.vector_results)} documents + {len(rag_result.graph_results)} entit√©s graphiques

INSTRUCTIONS :
- R√©ponds de mani√®re naturelle et amicale en fran√ßais
- Utilise prioritairement le contexte fourni
- Si le score de confiance est √©lev√© (>0.7), sois confiant dans ta r√©ponse
- Si le score est moyen (0.4-0.7), mentionne que tu as des informations partielles
- Si le score est faible (<0.4), sois honn√™te sur les limitations
- Reste focalis√© sur TeamSquare et ses services
- Sois concis mais informatif (maximum 300 mots)

STYLE :
- Conversationnel et humain
- Professionnel mais accessible
- Utilise des emojis avec parcimonie"""
        
        try:
            response = self.llm_manager.generate(query, system_prompt)
            
            # V√©rifier la qualit√© de la r√©ponse
            if len(response) < 20 or "erreur" in response.lower():
                return self._generate_fallback(query, rag_result.combined_context, query_type)
            
            return response
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur g√©n√©ration LLM: {e}")
            return self._generate_fallback(query, rag_result.combined_context, query_type)
    
    def _generate_fallback(self, query: str, context: str, query_type: str) -> str:
        """G√©n√©ration de fallback sans LLM"""
        
        if query_type == "greeting":
            return "Bonjour ! Je suis votre assistant TeamSquare avec recherche avanc√©e. Comment puis-je vous aider aujourd'hui ?"
        
        elif query_type == "pricing":
            return """Voici nos tarifs TeamSquare :

**Plan Starter** - 29‚Ç¨/mois
‚Ä¢ Jusqu'√† 10 utilisateurs
‚Ä¢ Fonctionnalit√©s de base
‚Ä¢ Support email

**Plan Professional** - 79‚Ç¨/mois  
‚Ä¢ Jusqu'√† 50 utilisateurs
‚Ä¢ Fonctionnalit√©s avanc√©es
‚Ä¢ API incluse
‚Ä¢ Support prioritaire

**Plan Enterprise** - 199‚Ç¨/mois
‚Ä¢ Utilisateurs illimit√©s
‚Ä¢ Toutes les fonctionnalit√©s
‚Ä¢ Support d√©di√© 24/7
‚Ä¢ Personnalisations

Quel plan vous int√©resse le plus ?"""
        
        elif query_type == "teamsquare":
            return """TeamSquare est une plateforme de collaboration moderne qui permet aux √©quipes de :

üöÄ **Collaborer en temps r√©el** avec chat int√©gr√©
üìä **G√©rer des projets** de mani√®re intuitive et visuelle  
üìÅ **Partager des fichiers** en toute s√©curit√©
üìà **Analyser les performances** avec des tableaux de bord
üîå **Int√©grer facilement** gr√¢ce √† notre API compl√®te
üõ°Ô∏è **S√©curiser les donn√©es** avec un niveau entreprise

Que souhaitez-vous savoir de plus sur TeamSquare ?"""
        
        else:
            if context and len(context) > 100:
                return f"Bas√© sur mes recherches avanc√©es :\n\n{context[:500]}...\n\nAvez-vous des questions plus sp√©cifiques ?"
            else:
                return f"Je comprends votre question sur '{query}'. Pouvez-vous √™tre plus sp√©cifique ? Je peux vous aider avec les prix, fonctionnalit√©s, et services TeamSquare."

# Test rapide
def test_agent():
    """Test de l'agent RAG hybride"""
    print("üß™ TEST AGENT RAG HYBRIDE (Vector + Graph)")
    print("-" * 60)
    
    agent = AgenticSupportAgentRAGGraph()
    
    test_queries = [
        "bonjour",
        "donner moi les prix",
        "qu'est-ce que TeamSquare ?",
        "comment fonctionne l'API TeamSquare ?",
        "quelles sont les int√©grations disponibles ?"
    ]
    
    for query in test_queries:
        print(f"\nüîç Test: {query}")
        response = agent.process_query(query)
        print(f"üìù R√©ponse: {response[:200]}...")

if __name__ == "__main__":
    test_agent()
