import os
import json
import time
import requests
from typing import Dict, Any, Optional
from datetime import datetime, timedelta

class HybridLLMManager:
    """Gestionnaire LLM hybride : OpenAI -> Mistral 7B"""
    
    def __init__(self):
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.ollama_url = "http://localhost:11434"
        self.mistral_model = "mistral:7b-instruct"
        
        # Configuration des limites (ajustables)
        self.token_limit_per_hour = 8000   # Limite tokens/heure (conservateur)
        self.token_limit_per_day = 40000   # Limite tokens/jour (conservateur)
        
        # Tracking des tokens
        self.token_usage_file = "config/token_usage.json"
        self.load_token_usage()
        
        # √âtat du syst√®me - OpenAI disponible si cl√© pr√©sente
        self.openai_available = bool(self.openai_api_key)
        self.mistral_available = self._check_mistral()
        self.current_provider = "openai" if self.openai_available else "mistral"
        
        print(f"ü§ñ LLM Manager initialis√©")
        print(f"   üîë OpenAI: {'‚úÖ Disponible' if self.openai_available else '‚ùå Pas de cl√©'}")
        print(f"   ü¶ô Mistral: {'‚úÖ Disponible' if self.mistral_available else '‚ùå Non install√©'}")
        print(f"   üéØ Provider actuel: {self.current_provider}")
    
    def _check_mistral(self) -> bool:
        """V√©rifie la disponibilit√© de Mistral (sans test co√ªteux)"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=3)
            if response.status_code == 200:
                models = response.json().get("models", [])
                for model in models:
                    if "mistral" in model.get("name", "").lower():
                        return True
            return False
        except:
            return False
    
    def load_token_usage(self):
        """Charge l'utilisation des tokens"""
        try:
            if os.path.exists(self.token_usage_file):
                with open(self.token_usage_file, 'r') as f:
                    self.token_usage = json.load(f)
            else:
                self.token_usage = {
                    "daily": {"date": "", "tokens": 0},
                    "hourly": {"hour": "", "tokens": 0}
                }
        except:
            self.token_usage = {
                "daily": {"date": "", "tokens": 0},
                "hourly": {"hour": "", "tokens": 0}
            }
    
    def save_token_usage(self):
        """Sauvegarde l'utilisation des tokens"""
        try:
            os.makedirs(os.path.dirname(self.token_usage_file), exist_ok=True)
            with open(self.token_usage_file, 'w') as f:
                json.dump(self.token_usage, f, indent=2)
        except:
            pass
    
    def update_token_usage(self, tokens: int):
        """Met √† jour le compteur de tokens"""
        now = datetime.now()
        today = now.strftime("%Y-%m-%d")
        current_hour = now.strftime("%Y-%m-%d-%H")
        
        # Reset daily counter if new day
        if self.token_usage["daily"]["date"] != today:
            self.token_usage["daily"] = {"date": today, "tokens": 0}
        
        # Reset hourly counter if new hour
        if self.token_usage["hourly"]["hour"] != current_hour:
            self.token_usage["hourly"] = {"hour": current_hour, "tokens": 0}
        
        # Add tokens
        self.token_usage["daily"]["tokens"] += tokens
        self.token_usage["hourly"]["tokens"] += tokens
        
        self.save_token_usage()
        
        # Log usage
        print(f"üìä Tokens utilis√©s: +{tokens} (Total jour: {self.token_usage['daily']['tokens']}, Heure: {self.token_usage['hourly']['tokens']})")
    
    def should_use_openai(self) -> bool:
        """D√©termine si on peut utiliser OpenAI"""
        if not self.openai_available:
            return False
        
        # V√©rifier les limites de tokens
        if self.token_usage["hourly"]["tokens"] >= self.token_limit_per_hour:
            print(f"‚ö†Ô∏è Limite horaire atteinte ({self.token_usage['hourly']['tokens']}/{self.token_limit_per_hour})")
            return False
        
        if self.token_usage["daily"]["tokens"] >= self.token_limit_per_day:
            print(f"‚ö†Ô∏è Limite quotidienne atteinte ({self.token_usage['daily']['tokens']}/{self.token_limit_per_day})")
            return False
        
        return True
    
    def generate_with_openai(self, prompt: str, system_prompt: str = None) -> str:
        """G√©n√®re avec OpenAI"""
        try:
            from langchain_openai import ChatOpenAI
            from langchain.schema import HumanMessage, SystemMessage
            
            llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=0.1,
                max_tokens=500,  # Limite pour contr√¥ler les co√ªts
                openai_api_key=self.openai_api_key
            )
            
            messages = []
            if system_prompt:
                messages.append(SystemMessage(content=system_prompt))
            messages.append(HumanMessage(content=prompt))
            
            print("üîÑ G√©n√©ration avec OpenAI...")
            response = llm.invoke(messages)
            
            # Estimer les tokens utilis√©s (approximation)
            estimated_tokens = len(prompt.split()) + len(response.content.split()) + (len(system_prompt.split()) if system_prompt else 0)
            self.update_token_usage(estimated_tokens)
            
            print(f"‚úÖ R√©ponse OpenAI g√©n√©r√©e ({len(response.content)} chars)")
            return response.content
            
        except Exception as e:
            error_str = str(e).lower()
            print(f"‚ùå Erreur OpenAI: {e}")
            
            # V√©rifier si c'est vraiment un probl√®me de quota
            if any(keyword in error_str for keyword in ["quota", "429", "rate limit", "insufficient"]):
                print("üîÑ Quota OpenAI √©puis√©, basculement vers Mistral")
                self.openai_available = False  # D√©sactiver temporairement
                return self.generate_with_mistral(prompt, system_prompt)
            elif any(keyword in error_str for keyword in ["401", "invalid", "unauthorized"]):
                print("üîÑ Cl√© OpenAI invalide, basculement vers Mistral")
                self.openai_available = False
                return self.generate_with_mistral(prompt, system_prompt)
            else:
                # Autre erreur, essayer Mistral
                print("üîÑ Erreur OpenAI, tentative avec Mistral")
                return self.generate_with_mistral(prompt, system_prompt)
    
    def generate_with_mistral(self, prompt: str, system_prompt: str = None) -> str:
        """G√©n√®re avec Mistral 7B"""
        try:
            # Format Mistral avec instructions
            if system_prompt:
                full_prompt = f"<s>[INST] {system_prompt}\n\nQuestion: {prompt} [/INST]"
            else:
                full_prompt = f"<s>[INST] {prompt} [/INST]"
            
            payload = {
                "model": self.mistral_model,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "max_tokens": 500
                }
            }
            
            print("ü¶ô G√©n√©ration avec Mistral 7B...")
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("response", "Erreur de g√©n√©ration")
                print(f"‚úÖ R√©ponse Mistral g√©n√©r√©e ({len(content)} chars)")
                return content
            else:
                return f"Erreur Mistral HTTP {response.status_code}"
                
        except Exception as e:
            print(f"‚ùå Erreur Mistral: {e}")
            return f"D√©sol√©, j'ai rencontr√© un probl√®me technique. Erreur: {str(e)}"
    
    def generate(self, prompt: str, system_prompt: str = None) -> str:
        """G√©n√®re une r√©ponse avec le meilleur provider disponible"""
        
        # Essayer OpenAI en premier si disponible et dans les limites
        if self.should_use_openai():
            try:
                self.current_provider = "openai"
                return self.generate_with_openai(prompt, system_prompt)
            except Exception as e:
                print(f"‚ö†Ô∏è √âchec OpenAI: {e}")
        
        # Fallback vers Mistral
        if self.mistral_available:
            self.current_provider = "mistral"
            return self.generate_with_mistral(prompt, system_prompt)
        else:
            return "‚ùå Aucun LLM disponible. V√©rifiez votre configuration OpenAI ou installez Mistral avec Ollama."
    
    def reset_openai_availability(self):
        """R√©active OpenAI (utile apr√®s une pause)"""
        if self.openai_api_key:
            self.openai_available = True
            print("üîÑ OpenAI r√©activ√©")
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le statut du syst√®me"""
        return {
            "openai_available": self.openai_available,
            "mistral_available": self.mistral_available,
            "current_provider": self.current_provider,
            "token_usage": self.token_usage,
            "limits": {
                "hourly": self.token_limit_per_hour,
                "daily": self.token_limit_per_day
            },
            "usage_percentage": {
                "hourly": round((self.token_usage["hourly"]["tokens"] / self.token_limit_per_hour) * 100, 1),
                "daily": round((self.token_usage["daily"]["tokens"] / self.token_limit_per_day) * 100, 1)
            }
        }

# Instance globale
hybrid_llm = HybridLLMManager()
