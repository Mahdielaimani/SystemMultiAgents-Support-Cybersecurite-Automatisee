import os
import json
import time
import requests
from typing import Dict, Any, Optional
from datetime import datetime, timedelta

class HybridLLMManagerGemini:
    """Gestionnaire LLM hybride : Google Gemini -> Mistral 7B"""
    
    def __init__(self):
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.gemini_model = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
        self.ollama_url = "http://localhost:11434"
        self.mistral_model = "mistral:7b-instruct"
        
        # Configuration des limites (Gemini a des quotas g√©n√©reux)
        self.token_limit_per_hour = 15000   # Limite tokens/heure
        self.token_limit_per_day = 100000   # Limite tokens/jour
        
        # Tracking des tokens
        self.token_usage_file = "config/token_usage_gemini.json"
        self.load_token_usage()
        
        # √âtat du syst√®me - Gemini disponible si cl√© pr√©sente
        self.gemini_available = bool(self.google_api_key)
        self.mistral_available = self._check_mistral()
        self.current_provider = "gemini" if self.gemini_available else "mistral"
        
        print(f"ü§ñ LLM Manager Gemini initialis√©")
        print(f"   üîë Gemini: {'‚úÖ Disponible' if self.gemini_available else '‚ùå Pas de cl√©'}")
        print(f"   ü¶ô Mistral: {'‚úÖ Disponible' if self.mistral_available else '‚ùå Non install√©'}")
        print(f"   üéØ Provider actuel: {self.current_provider}")
    
    def _check_mistral(self) -> bool:
        """V√©rifie la disponibilit√© de Mistral"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=3)
            if response.status_code == 200:
                models = response.json().get("models", [])
                for model in models:
                    if "mistral" in model.get("name", "").lower():
                        return True
            return False
        except:
            return False
    
    def load_token_usage(self):
        """Charge l'utilisation des tokens"""
        try:
            if os.path.exists(self.token_usage_file):
                with open(self.token_usage_file, 'r') as f:
                    self.token_usage = json.load(f)
            else:
                self.token_usage = {
                    "daily": {"date": "", "tokens": 0},
                    "hourly": {"hour": "", "tokens": 0}
                }
        except:
            self.token_usage = {
                "daily": {"date": "", "tokens": 0},
                "hourly": {"hour": "", "tokens": 0}
            }
    
    def save_token_usage(self):
        """Sauvegarde l'utilisation des tokens"""
        try:
            os.makedirs(os.path.dirname(self.token_usage_file), exist_ok=True)
            with open(self.token_usage_file, 'w') as f:
                json.dump(self.token_usage, f, indent=2)
        except:
            pass
    
    def update_token_usage(self, tokens: int):
        """Met √† jour le compteur de tokens"""
        now = datetime.now()
        today = now.strftime("%Y-%m-%d")
        current_hour = now.strftime("%Y-%m-%d-%H")
        
        # Reset daily counter if new day
        if self.token_usage["daily"]["date"] != today:
            self.token_usage["daily"] = {"date": today, "tokens": 0}
        
        # Reset hourly counter if new hour
        if self.token_usage["hourly"]["hour"] != current_hour:
            self.token_usage["hourly"] = {"hour": current_hour, "tokens": 0}
        
        # Add tokens
        self.token_usage["daily"]["tokens"] += tokens
        self.token_usage["hourly"]["tokens"] += tokens
        
        self.save_token_usage()
        
        # Log usage
        print(f"üìä Tokens utilis√©s: +{tokens} (Total jour: {self.token_usage['daily']['tokens']}, Heure: {self.token_usage['hourly']['tokens']})")
    
    def should_use_gemini(self) -> bool:
        """D√©termine si on peut utiliser Gemini"""
        if not self.gemini_available:
            return False
        
        # V√©rifier les limites de tokens
        if self.token_usage["hourly"]["tokens"] >= self.token_limit_per_hour:
            print(f"‚ö†Ô∏è Limite horaire atteinte ({self.token_usage['hourly']['tokens']}/{self.token_limit_per_hour})")
            return False
        
        if self.token_usage["daily"]["tokens"] >= self.token_limit_per_day:
            print(f"‚ö†Ô∏è Limite quotidienne atteinte ({self.token_usage['daily']['tokens']}/{self.token_limit_per_day})")
            return False
        
        return True
    
    def generate_with_gemini(self, prompt: str, system_prompt: str = None) -> str:
        """G√©n√®re avec Google Gemini"""
        try:
            import google.generativeai as genai
            
            # Configuration Gemini
            genai.configure(api_key=self.google_api_key)
            model = genai.GenerativeModel(self.gemini_model)
            
            # Construire le prompt complet
            if system_prompt:
                full_prompt = f"{system_prompt}\n\nQuestion: {prompt}"
            else:
                full_prompt = prompt
            
            print("üîÑ G√©n√©ration avec Google Gemini...")
            
            # Configuration de g√©n√©ration
            generation_config = genai.types.GenerationConfig(
                temperature=0.1,
                max_output_tokens=500,
                top_p=0.9,
            )
            
            response = model.generate_content(
                full_prompt,
                generation_config=generation_config
            )
            
            if response.text:
                # Estimer les tokens utilis√©s
                estimated_tokens = len(prompt.split()) + len(response.text.split()) + (len(system_prompt.split()) if system_prompt else 0)
                self.update_token_usage(estimated_tokens)
                
                print(f"‚úÖ R√©ponse Gemini g√©n√©r√©e ({len(response.text)} chars)")
                return response.text
            else:
                return "Erreur: R√©ponse vide de Gemini"
            
        except Exception as e:
            error_str = str(e).lower()
            print(f"‚ùå Erreur Gemini: {e}")
            
            # V√©rifier si c'est un probl√®me de quota
            if any(keyword in error_str for keyword in ["quota", "429", "rate limit", "resource_exhausted"]):
                print("üîÑ Quota Gemini √©puis√©, basculement vers Mistral")
                self.gemini_available = False
                return self.generate_with_mistral(prompt, system_prompt)
            elif any(keyword in error_str for keyword in ["401", "invalid", "unauthorized", "api_key"]):
                print("üîÑ Cl√© Gemini invalide, basculement vers Mistral")
                self.gemini_available = False
                return self.generate_with_mistral(prompt, system_prompt)
            else:
                print("üîÑ Erreur Gemini, tentative avec Mistral")
                return self.generate_with_mistral(prompt, system_prompt)
    
    def generate_with_mistral(self, prompt: str, system_prompt: str = None) -> str:
        """G√©n√®re avec Mistral 7B"""
        try:
            # Format Mistral avec instructions
            if system_prompt:
                full_prompt = f"<s>[INST] {system_prompt}\n\nQuestion: {prompt} [/INST]"
            else:
                full_prompt = f"<s>[INST] {prompt} [/INST]"
            
            payload = {
                "model": self.mistral_model,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "max_tokens": 500
                }
            }
            
            print("ü¶ô G√©n√©ration avec Mistral 7B...")
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("response", "Erreur de g√©n√©ration")
                print(f"‚úÖ R√©ponse Mistral g√©n√©r√©e ({len(content)} chars)")
                return content
            else:
                return f"Erreur Mistral HTTP {response.status_code}"
                
        except Exception as e:
            print(f"‚ùå Erreur Mistral: {e}")
            return f"D√©sol√©, j'ai rencontr√© un probl√®me technique. Erreur: {str(e)}"
    
    def generate(self, prompt: str, system_prompt: str = None) -> str:
        """G√©n√®re une r√©ponse avec le meilleur provider disponible"""
        
        # Essayer Gemini en premier si disponible et dans les limites
        if self.should_use_gemini():
            try:
                self.current_provider = "gemini"
                return self.generate_with_gemini(prompt, system_prompt)
            except Exception as e:
                print(f"‚ö†Ô∏è √âchec Gemini: {e}")
        
        # Fallback vers Mistral
        if self.mistral_available:
            self.current_provider = "mistral"
            return self.generate_with_mistral(prompt, system_prompt)
        else:
            return "‚ùå Aucun LLM disponible. V√©rifiez votre configuration Gemini ou installez Mistral avec Ollama."
    
    def reset_gemini_availability(self):
        """R√©active Gemini (utile apr√®s une pause)"""
        if self.google_api_key:
            self.gemini_available = True
            print("üîÑ Gemini r√©activ√©")
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le statut du syst√®me"""
        return {
            "gemini_available": self.gemini_available,
            "mistral_available": self.mistral_available,
            "current_provider": self.current_provider,
            "token_usage": self.token_usage,
            "limits": {
                "hourly": self.token_limit_per_hour,
                "daily": self.token_limit_per_day
            },
            "usage_percentage": {
                "hourly": round((self.token_usage["hourly"]["tokens"] / self.token_limit_per_hour) * 100, 1),
                "daily": round((self.token_usage["daily"]["tokens"] / self.token_limit_per_day) * 100, 1)
            }
        }

# Instance globale
hybrid_llm_gemini = HybridLLMManagerGemini()
